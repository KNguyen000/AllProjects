{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt     # for plotting\n",
    "import cv2 \n",
    "import soundfile as sf\n",
    "import timeit\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path:\n",
    "data_path = '../data/'\n",
    "speakers_path = '../SPEAKERS.TXT'\n",
    "\n",
    "sample_rate = 32000\n",
    "off_set = 0\n",
    "duration = 5.0\n",
    "\n",
    "# If you decide to use batch sizes:\n",
    "batch_size = 10000\n",
    "\n",
    "sample_size = (round((sample_rate*duration)), 1)\n",
    "\n",
    "#Mel features\n",
    "hop_length = 512 #the default spacing between frames, try 512?\n",
    "n_fft = 1024 #number of samples\n",
    "n_mels = 64\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_sex():\n",
    "    id_sex_array = []\n",
    "    with open(speakers_path, 'r') as file:\n",
    "        # Skip the header line\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # Split the line into columns\n",
    "            columns = line.split('|')\n",
    "            # Extract the ID and SEX\n",
    "            id_value = columns[0].strip()\n",
    "            sex_value = columns[1].strip()\n",
    "            # Append ID and SEX to the array\n",
    "            id_sex_array.append((id_value, sex_value))\n",
    "    return id_sex_array\n",
    "\n",
    "# Lets store the speaker gender, it'll only be used once.\n",
    "speaker_gender = extract_id_sex()\n",
    "\n",
    "def load_directory():\n",
    "    # Find all samples in the directory\n",
    "    files = glob(os.path.join(data_path, '*.flac'))\n",
    "    # The audio data\n",
    "    x = []\n",
    "    # The gender and speaker ID\n",
    "    y = []\n",
    "\n",
    "    # Create a dictionary for speaker genders\n",
    "    speaker_gender_dict = {speaker[0]: speaker[1] for speaker in speaker_gender}\n",
    "\n",
    "    counter_f = 0\n",
    "    counter_m = 0\n",
    "    speakers = []\n",
    "\n",
    "    if batch_size > len(files):\n",
    "        error_msg = f'Requested a batch size greater than the number of samples available ({len(files)})'\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    for file in files:\n",
    "        if (len(x) == batch_size):\n",
    "            break\n",
    "        \n",
    "        # Collect the speaker IDs, because we are classifying on gender the speakerID does not \n",
    "        # matter as a label, but it does help us ensure we have a diverse representation of \n",
    "        # speakers for each gender, so we will only take 1 sample per speaker for each gender.\n",
    "        fileName = file.split('\\\\')[1]\n",
    "        speakerId = fileName.split('-')[0]\n",
    "\n",
    "        # Due to limited number of samples per speaker limit the below code to 14500\n",
    "        if batch_size < 14500:\n",
    "            # extract an even number of samples from each speaker ensuring we get a diverse\n",
    "            # range of speakers \n",
    "            num_samples_per_speaker = round(batch_size / 250)\n",
    "            speaker_count = Counter(speakers)\n",
    "            if (speaker_count[speakerId] >= num_samples_per_speaker):\n",
    "                continue\n",
    "        \n",
    "        # Use dictionary for faster search performance.\n",
    "        gender = speaker_gender_dict.get(speakerId)\n",
    "\n",
    "        if (gender == 'F' and (counter_f < batch_size/2)):\n",
    "            data, _ = librosa.load(file, sr=sample_rate, offset=off_set, duration=duration)\n",
    "            \n",
    "            if (data.size < (sample_rate*duration)):\n",
    "                continue\n",
    "            x.append(data)\n",
    "            y.append((gender, speakerId))\n",
    "            speakers.append(speakerId)\n",
    "            counter_f = counter_f + 1\n",
    "\n",
    "        elif (gender == 'M' and (counter_m < batch_size/2)):\n",
    "            data, _ = librosa.load(file, sr=sample_rate, offset=off_set, duration=duration)\n",
    "            if (data.size < (sample_rate*duration)):\n",
    "                continue\n",
    "            x.append(data)\n",
    "            y.append((gender, speakerId))\n",
    "            speakers.append(speakerId)\n",
    "            counter_m = counter_m + 1\n",
    "    x = [get_mels(x_data, sample_rate, hop_length, n_fft, n_mels).T  for x_data in x]\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "\n",
    "def get_mels(x, sample_rate, hop_length=512, n_fft=1024, n_mels=64):\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y =x, sr = sample_rate, hop_length=hop_length,n_fft = n_fft, n_mels=n_mels)\n",
    "    S_db_mel = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_db_mel\n",
    "\n",
    "\n",
    "# Convert iterable into batches of size n\n",
    "#   iterable: original list of data to be batched\n",
    "#   n: batch size \n",
    "\n",
    "#   yields:   a list of batches with each batch of size n\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "# Get y label and a unique ID for each file and load the file into a tuple\n",
    "#   file: directory to the file\n",
    "\n",
    "#   returns:   a tuple containing the audio file, speaker ID and a unique ID for the file name\n",
    "def get_metadata(file):\n",
    "    basename = os.path.basename(file)\n",
    "    data, samp_rate = librosa.load(file, sr=sample_rate)\n",
    "    speakerId = basename.split('-')[0]\n",
    "    uniqueId = basename.split('.')[0]\n",
    "    return (data, uniqueId), speakerId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_sequences(X_train, 20, 20, 5, 128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x data, first sample: [[-27.368961 -29.47931  -36.750004 ... -75.74742  -75.96059  -76.09476 ]\n",
      " [-22.879776 -30.233055 -49.52315  ... -80.       -80.       -80.      ]\n",
      " [-21.997183 -28.55328  -47.38311  ... -80.       -80.       -80.      ]\n",
      " ...\n",
      " [-26.280758 -35.079685 -44.866646 ... -80.       -80.       -80.      ]\n",
      " [-17.730782 -24.628962 -51.294506 ... -80.       -80.       -80.      ]\n",
      " [-16.015625 -21.281998 -37.17293  ... -80.       -80.       -80.      ]]\n",
      "shape x data: (10000, 313, 64)\n",
      "shape y data: (10000,)\n",
      "\n",
      "Male Speakers: 126\n",
      "Female Speakers: 125\n"
     ]
    }
   ],
   "source": [
    "x_data, y = load_directory()\n",
    "# Information on the extracted audio data:\n",
    "print(f'x data, first sample: {x_data[0]}')\n",
    "print(f'shape x data: {x_data.shape}')\n",
    "\n",
    "y_data = np.array([row[0] for row in y])\n",
    "speakerIds = [row[1] for row in y]\n",
    "print(f'shape y data: {y_data.shape}')\n",
    "\n",
    "m_rows = y[y[:, 0] == 'M']\n",
    "f_rows = y[y[:, 0] == 'F']\n",
    "print()\n",
    "print(f'Male Speakers: {len(np.unique(m_rows[:, 1]))}')\n",
    "print(f'Female Speakers: {len(np.unique(f_rows[:, 1]))}')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array((X-np.min(X))/(np.max(X)-np.min(X)))\n",
    "# X = X/np.std(X)\n",
    "# y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_remain, y_train, y_remain = train_test_split(x_data, y_data, \n",
    "                                                        test_size=0.6, \n",
    "                                                        random_state=42, \n",
    "                                                        shuffle=True,\n",
    "                                                        stratify=y_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_remain, y_remain, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=42, \n",
    "                                                shuffle=True,\n",
    "                                                stratify=y_remain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n",
      "1\n",
      "M\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Convert 'M' (1) and 'F' (0) to numbers. \n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "print(y_train[0])\n",
    "print(y_train_encoded[0])\n",
    "print(y_train[90])\n",
    "print(y_train_encoded[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " LSTM_1 (LSTM)               (None, 70)                37800     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1136      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,953\n",
      "Trainable params: 38,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(313, n_mels)\n",
    "model = keras.Sequential()\n",
    "model.add(LSTM(70,input_shape=input_shape, name='LSTM_1'), return_sequences=True)\n",
    "model.add(attention())\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='BinaryCrossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'F', 'M', ..., 'F', 'M', 'F'], dtype='<U1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "40/40 [==============================] - 23s 480ms/step - loss: 0.6461 - acc: 0.6665 - val_loss: 0.5814 - val_acc: 0.8133\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 18s 453ms/step - loss: 0.5559 - acc: 0.8282 - val_loss: 0.5223 - val_acc: 0.8507\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 18s 460ms/step - loss: 0.4570 - acc: 0.8630 - val_loss: 0.3928 - val_acc: 0.8733\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 19s 466ms/step - loss: 0.3814 - acc: 0.8670 - val_loss: 0.3235 - val_acc: 0.8897\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 19s 475ms/step - loss: 0.3564 - acc: 0.8765 - val_loss: 0.3826 - val_acc: 0.8787\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 19s 488ms/step - loss: 0.3267 - acc: 0.8817 - val_loss: 0.3296 - val_acc: 0.8777\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 19s 481ms/step - loss: 0.3022 - acc: 0.8882 - val_loss: 0.3255 - val_acc: 0.8997\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 23s 585ms/step - loss: 0.3064 - acc: 0.8928 - val_loss: 0.2832 - val_acc: 0.8997\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 26s 638ms/step - loss: 0.2930 - acc: 0.8978 - val_loss: 0.2781 - val_acc: 0.8997\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 20s 498ms/step - loss: 0.2694 - acc: 0.9035 - val_loss: 0.2507 - val_acc: 0.9180\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 20s 493ms/step - loss: 0.2598 - acc: 0.9082 - val_loss: 0.2532 - val_acc: 0.9087\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 19s 488ms/step - loss: 0.2450 - acc: 0.9135 - val_loss: 0.2370 - val_acc: 0.9197\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 20s 509ms/step - loss: 0.2288 - acc: 0.9165 - val_loss: 0.2279 - val_acc: 0.9053\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 19s 487ms/step - loss: 0.2253 - acc: 0.9143 - val_loss: 0.2127 - val_acc: 0.9303\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 20s 510ms/step - loss: 0.2100 - acc: 0.9230 - val_loss: 0.1969 - val_acc: 0.9283\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 19s 474ms/step - loss: 0.2068 - acc: 0.9212 - val_loss: 0.2076 - val_acc: 0.9247\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 20s 495ms/step - loss: 0.2250 - acc: 0.9175 - val_loss: 0.2259 - val_acc: 0.9153\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 20s 493ms/step - loss: 0.2188 - acc: 0.9187 - val_loss: 0.2291 - val_acc: 0.9160\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 20s 505ms/step - loss: 0.1971 - acc: 0.9270 - val_loss: 0.2048 - val_acc: 0.9200\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 23s 574ms/step - loss: 0.2022 - acc: 0.9247 - val_loss: 0.1911 - val_acc: 0.9313\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 20s 501ms/step - loss: 0.1946 - acc: 0.9287 - val_loss: 0.1928 - val_acc: 0.9303\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 19s 485ms/step - loss: 0.2035 - acc: 0.9293 - val_loss: 0.2292 - val_acc: 0.9047\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 19s 488ms/step - loss: 0.2073 - acc: 0.9265 - val_loss: 0.2002 - val_acc: 0.9240\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 19s 478ms/step - loss: 0.2215 - acc: 0.9145 - val_loss: 0.1905 - val_acc: 0.9310\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2170 - acc: 0.9190 - val_loss: 0.1891 - val_acc: 0.9343\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 19s 482ms/step - loss: 0.1996 - acc: 0.9302 - val_loss: 0.2106 - val_acc: 0.9127\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 20s 498ms/step - loss: 0.1946 - acc: 0.9277 - val_loss: 0.1951 - val_acc: 0.9230\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 18s 462ms/step - loss: 0.2010 - acc: 0.9258 - val_loss: 0.1923 - val_acc: 0.9283\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 18s 443ms/step - loss: 0.2005 - acc: 0.9273 - val_loss: 0.1833 - val_acc: 0.9320\n",
      "Epoch 30/100\n",
      " 6/40 [===>..........................] - ETA: 12s - loss: 0.1728 - acc: 0.9333"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "history = model.fit(x_train, y_train_encoded, epochs=100, batch_size=100, validation_data=(x_val, y_val_encoded), shuffle=False, verbose=True)\n",
    "end = time.time()\n",
    "model_training = end - start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "101/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[15, 7])\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(history.history['loss'], label='training loss')\n",
    "ax.plot(history.history['val_loss'], label='validation loss')\n",
    "ax.set_title('Training and Validation Loss - 1 Layer LSTM')\n",
    "ax.legend()\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(history.history['acc'], label='Model Training Set Accuracy')\n",
    "ax.plot(history.history['val_acc'], label='Model Validation Set Accuracy')\n",
    "ax.set_title('Training and Validation Accuracy - 1 Layer LSTM')\n",
    "ax.legend()\n",
    "plt.savefig('../out/LSTM_model_loss_1_layer.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score\n",
    "\n",
    "pred_lab = model.predict(x_train);\n",
    "train_indexes = np.round(pred_lab)\n",
    "\n",
    "pred_lab = model.predict(x_val);\n",
    "val_indexes = np.round(pred_lab)\n",
    "\n",
    "start = time.time()\n",
    "pred_lab = model.predict(x_test);\n",
    "test_indexes = np.round(pred_lab)\n",
    "\n",
    "f1_train = f1_score(y_train_encoded, train_indexes)\n",
    "f1_val = f1_score(y_val_encoded, val_indexes)\n",
    "f1_test = f1_score(y_test_encoded, test_indexes)\n",
    "\n",
    "\n",
    "acc_train = accuracy_score(y_train_encoded, train_indexes)\n",
    "acc_val = accuracy_score(y_val_encoded, val_indexes)\n",
    "acc_test = accuracy_score(y_test_encoded, test_indexes)\n",
    "\n",
    "end = time.time()\n",
    "model_inference = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "fig = plt.figure(figsize=[18, 7])\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.set_title(f\"Training Dataset - f1 = {round(f1_train, 3)}\")\n",
    "cm = confusion_matrix(y_train_encoded, train_indexes, normalize='true')\n",
    "c = ConfusionMatrixDisplay(cm)\n",
    "c.plot(ax = ax, xticks_rotation = 'vertical')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.set_title(f\"Validation Dataset - f1 = {round(f1_val, 3)}\")\n",
    "cm = confusion_matrix(y_val_encoded, val_indexes, normalize='true')\n",
    "c = ConfusionMatrixDisplay(cm)\n",
    "c.plot(ax = ax, xticks_rotation = 'vertical')\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.set_title(f\"Testing Dataset - f1 = {round(f1_test,3)}\")\n",
    "cm = confusion_matrix(y_test_encoded, test_indexes, normalize='true')\n",
    "c = ConfusionMatrixDisplay(cm)\n",
    "c.plot(ax = ax,  xticks_rotation = 'vertical')\n",
    "\n",
    "plt.savefig('../out/LSTM_1_layer_Confusion_Matrix.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(history, open('../out/LSTM_1_layer_history.pkl', 'wb'))\n",
    "pickle.dump(model, open('../out/LSTM_1_layer_Model_speaker.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "test_embeddings = model.predict(x_test)\n",
    "tsne_embeddings = TSNE(random_state=4).fit_transform(test_embeddings)\n",
    "fig = plt.figure(figsize=[12, 12])\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = y_test_encoded.flatten());\n",
    "plt.savefig('../out/LSTM_1_layer_clusters.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pickle.load(open('../out/LSTM_1_layer_Model_speaker.pkl', 'rb'))\n",
    "# with open('../out/LSTM_1_layer_history.pkl', \"rb\") as file_pi:\n",
    "#     history = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
